{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe5DT0yorkbZYztg2Ei7x6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabitza-tech/ETTI-SummerSchool2025/blob/main/Students_MachineLearning_Intro_MultiClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ™‚ðŸ˜  Labeled Faces in the Wild (LFW) Dataset\n",
        "\n",
        "The **Labeled Faces in the Wild (LFW)** dataset is a widely used benchmark for face recognition and classification tasks. It contains **13,233 images of 5,749 individuals**, captured in unconstrained real-world settings, which include variations in **pose, expression, lighting, and background**.  \n",
        "\n",
        "For many machine learning experiments, a **smaller subset** is used by filtering individuals with a minimum number of images (e.g., `min_faces_per_person=20`).\n",
        "\n",
        "Each image is **grayscale or RGB** and can be resized and flattened for use in standard classifiers.  \n",
        "\n",
        "Key challenges of LFW include:\n",
        "- **Class imbalance**: Some individuals have more images than others.\n",
        "- **Unaligned faces**: Variation in pose and expression.\n",
        "- **Nonlinear separability**: Requires models capable of capturing complex patterns.  \n",
        "\n",
        "LFW provides a realistic setting to test both **traditional machine learning algorithms** and **deep learning approaches** for face identification and verification.\n",
        "\n",
        "We will use this dataset for comparing models such as **K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multi-Layer Perceptrons (MLP)** on the face recognition task.\n",
        "\n",
        "Let's first take a look at the dataset and its images.\n",
        "\n",
        "# ðŸ§ª Exercise 1\n",
        "\n",
        "### Tasks\n",
        "1. Load the dataset with the default settings:\n",
        "   - **Minimum 20 faces per person**\n",
        "   - **Image resize factor: 0.4**\n",
        "2. Experiment with:\n",
        "   - **Different image resize factors**\n",
        "   - **Different values for minimum number of faces per person**\n",
        "\n",
        "   Observe and report:\n",
        "   - How many **additional classes** are added\n",
        "   - How many **additional samples** are included\n",
        "   - How the **image quality** changes with different resizing factors\n"
      ],
      "metadata": {
        "id": "i_KdMkwMVb9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "# Load LFW dataset (small subset)\n",
        "lfw = fetch_lfw_people(min_faces_per_person=20, resize=0.4)\n",
        "# The images in 2D format\n",
        "images = lfw.images\n",
        "# The actual name of the labels\n",
        "target_names = lfw.target_names\n",
        "# The flatten image (this is what we will work with) - X\n",
        "x = lfw.data   # flattened images\n",
        "# The labels values - we were calling these Y\n",
        "y = lfw.target\n",
        "\n",
        "# Number of images to display\n",
        "num_images = 12\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i in range(num_images):\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(images[i], cmap='gray')\n",
        "    plt.title(target_names[y[i]])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# CODE HERE\n",
        "# Print some statistics about the dataset, number of samples, number of classes, images size, flatten image size, etc.\n",
        "# What are the ranges of values for the data samples 'x'?\n"
      ],
      "metadata": {
        "id": "CM-ZiZzRTao4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing, PCA, and Data Splitting\n",
        "\n",
        "The dataset features are already normalized between **[0, 1]** (scaled grayscale pixel values),  \n",
        "so no further preprocessing is required.\n",
        "\n",
        "However, each sample has **over 1850 features** (pixel values) â€” which is quite high.  \n",
        "To address this, we use **Principal Component Analysis (PCA)** for dimensionality reduction.  \n",
        "This helps by:\n",
        "- Removing redundant features  \n",
        "- Reducing computational complexity and training time  \n",
        "- Potentially improving performance  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Exercise 2\n",
        "\n",
        "### Tasks\n",
        "1. Keep the dataset with:\n",
        "   - **Minimum 20 faces per person**\n",
        "   - **Resize factor: 0.4**\n",
        "2. Split the dataset into:\n",
        "   - **60% Train**\n",
        "   - **20% Validation**\n",
        "   - **20% Test**\n",
        "3. Apply **PCA** to reduce the number of features to **150 components**.\n",
        "4. Train a **KNN classifier (k = 5)**:\n",
        "   - Using **original features**\n",
        "   - Using **PCA-reduced features**\n",
        "   - Evaluate both on the **Validation set**, then on the **Test set**\n",
        "5. Check whether the performance trend on the **Validation set** holds for **unseen Test data**.\n",
        "6. Compare:\n",
        "   - **Training time**\n",
        "   - **Final Accuracy**\n",
        "   - **Weighted F1-score**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_XKLpg_ZXdUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE FOR DATASET SPLITTING HERE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "....\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]}, Validation: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n"
      ],
      "metadata": {
        "id": "APz9f9LrXdcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# CODE FOR PCA here\n",
        "# -------------------------\n",
        "# Apply PCA for dimensionality reduction\n",
        "# -------------------------\n",
        "\n",
        "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
        "X_train_pca = ...\n",
        "X_val_pca = ...\n",
        "X_test_pca = ...\n",
        "print(f\"PCA reduced feature shape: {X_train_pca.shape}\")"
      ],
      "metadata": {
        "id": "COApjrpaZcq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# CODE HERE THE TRAINING AND PREDICTION OF A KNN WITH AND WITHOUT PCA\n",
        "# No PCA\n",
        "...\n",
        "y_pred_val = ... # VALIDATION\n",
        "y_pred = ... #TEST\n",
        "\n",
        "# PCA\n",
        "...\n",
        "y_pred_val_pca = ... #VALIDATION\n",
        "y_pred_pca = ... #TEST\n",
        "\n",
        "# VALIDATION\n",
        "acc = ...\n",
        "f1_score_nopca = ...\n",
        "\n",
        "acc_pca = ...\n",
        "f1_score_pca = ...\n",
        "\n",
        "print(f\"Classification report VALIDATION for KNN with K=5 and no PCA: acc={acc},f1-score(weighted)={f1_score_nopca}\")\n",
        "print(f\"Classification report VALIDATION for KNN with K=5 and PCA: acc={acc_pca},f1-score(weighted)={f1_score_pca}\")\n",
        "\n",
        "# EVALUATION\n",
        "acc = ...\n",
        "f1_score_nopca = ...\n",
        "\n",
        "acc_pca = ...\n",
        "f1_score_pca = ...\n",
        "\n",
        "print(f\"\\nClassification report TEST for KNN with K=5 and no PCA: acc={acc},f1-score(weighted)={f1_score_nopca}\")\n",
        "print(f\"Classification report TEST for KNN with K=5 and PCA: acc={acc_pca},f1-score(weighted)={f1_score_pca}\")\n"
      ],
      "metadata": {
        "id": "wzKLiKLOZfyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”§ Tuning the Methods on a Validation Set - KNN\n",
        "\n",
        "Great! We have already seen that **PCA helps a lot** in terms of both **training time** and **performance**, so we will continue using it from now on.  \n",
        "\n",
        "Next, we want to improve the **KNN classifier** by tuning its hyper-parameters on the **validation set**.  \n",
        "Up until now, we chose the number of neighbors `K = 5` arbitrarily â€” this might not be the best choice.  \n",
        "\n",
        "A crucial step in Machine Learning is **hyper-parameter tuning**. Without a validation set (or a held-out portion of the data), choosing hyper-parameters is essentially guesswork:  \n",
        "- Sometimes you might get lucky and find a good value.  \n",
        "- Other times you could end up with subpar results.  \n",
        "\n",
        "To avoid this, we will perform a **Grid Search** over different values of `K`. Specifically, we will test `K âˆˆ [1, 3, 5, 7, 10, 13]`.  \n",
        "After identifying the best `K` based on validation performance, we will then evaluate this setting on the **Test set**.\n",
        "\n",
        "---\n",
        "\n",
        "# ðŸŽ¯ Exercise 3\n",
        "\n",
        "### Tasks\n",
        "1. Loop over all `K` values in `[1, 3, 5, 7, 10, 13]`, training and evaluating on the **Validation set**.  \n",
        "2. Record both **accuracy** and **weighted F1-score** for each `K`.  \n",
        "3. Select the **best K** from validation and evaluate it on the **Test set**.  \n",
        "4. Reflect:  \n",
        "   - Did tuning improve the results?  \n",
        "   - Or was our initial choice (`K = 5`) already close to optimal?  \n"
      ],
      "metadata": {
        "id": "XVwhb0DvcNZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# List of K values to try\n",
        "k_values = [1, 3, 5, 7, 10, 13]\n",
        "\n",
        "best_k = None\n",
        "best_f1 = 0\n",
        "\n",
        "# Iterate over K values and evaluate on validation set\n",
        "for k in k_values:\n",
        "    # Train KNN\n",
        "    ...\n",
        "\n",
        "    # Validate\n",
        "\n",
        "    # Metrics\n",
        "    f1_val = ...\n",
        "    acc_val = ...\n",
        "\n",
        "    print(f\"K={k}: Validation acc={acc_val:.4f}, f1(weighted)={f1_val:.4f}\")\n",
        "\n",
        "    # New best value?\n",
        "    if f1_val > best_f1:\n",
        "        best_f1 = f1_val\n",
        "        best_k = k\n",
        "\n",
        "print(f\"\\nBest K based on validation F1: K={best_k}\")\n",
        "\n",
        "# Train final model with best K on PCA features\n",
        "clf_best = ...\n",
        "...\n",
        "\n",
        "# Predict on validation and test sets\n",
        "y_val_best = ...\n",
        "y_test_best = ...\n",
        "\n",
        "# Validation metrics\n",
        "acc_val_best = ...\n",
        "f1_val_best = ...\n",
        "\n",
        "# Test metrics\n",
        "acc_test_best = ...\n",
        "f1_test_best = ...\n",
        "\n",
        "print(f\"\\nValidation (best K={best_k}): acc={acc_val_best:.4f}, f1(weighted)={f1_val_best:.4f}\")\n",
        "print(f\"Test (best K={best_k}): acc={acc_test_best:.4f}, f1(weighted)={f1_test_best:.4f}\")\n"
      ],
      "metadata": {
        "id": "SU_1Ft7Kef5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš¡ Support Vector Machines (SVM)\n",
        "\n",
        "Now letâ€™s try **Support Vector Machines (SVM)** as an alternative to KNN.  \n",
        "Unlike KNN, SVM has multiple hyper-parameters to tune, which strongly influence performance.  \n",
        "\n",
        "We will focus on two key parameters:\n",
        "- **`C`** (Regularization strength): `[0.1, 1, 10]`\n",
        "- **`gamma`** (Kernel coefficient): `['scale', 'auto']`\n",
        "\n",
        "This gives us a grid of possible `(C, gamma)` pairs to explore.  \n",
        "We will tune them using the **Validation set**, then test the best configuration on unseen data.\n",
        "\n",
        "---\n",
        "# Exercise 4\n",
        "### Tasks\n",
        "1. Train SVM models for all combinations of:  \n",
        "   - `C âˆˆ [0.1, 1, 10]`  \n",
        "   - `gamma âˆˆ ['scale', 'auto']`  \n",
        "2. Evaluate each model on the **Validation set** using **accuracy** and **weighted F1-score**.  \n",
        "3. Select the best-performing `(C, gamma)` pair.  \n",
        "4. Evaluate this best model on the **Test set** (accuracy + weighted F1-score).  \n",
        "5. Compare results:  \n",
        "   - How does SVM performance compare to **KNN**?  \n",
        "   - Did tuning `C` and `gamma` significantly change the results?  \n"
      ],
      "metadata": {
        "id": "VODhMWhofKK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- SVM hyperparameters to try ---\n",
        "C_values = [0.1, 1, 10]\n",
        "gamma_values = ['scale', 'auto']  # common options for gamma\n",
        "\n",
        "best_svm_params = None\n",
        "best_f1_svm = 0\n",
        "\n",
        "# Iterate over hyperparameters on validation set\n",
        "for C in C_values:\n",
        "    for gamma in gamma_values:\n",
        "        # TRAIN SVM\n",
        "        ...\n",
        "\n",
        "        # VALIDATE\n",
        "\n",
        "        #METRICS\n",
        "        f1_val = ...\n",
        "        acc_val = ...\n",
        "\n",
        "        print(f\"SVM C={C}, gamma={gamma}: Validation acc={acc_val:.4f}, f1(weighted)={f1_val:.4f}\")\n",
        "\n",
        "        # New best?\n",
        "        if f1_val > best_f1_svm:\n",
        "            best_f1_svm = f1_val\n",
        "            best_svm_params = {'C': C, 'gamma': gamma}\n",
        "\n",
        "print(f\"\\nBest SVM params based on validation F1: {best_svm_params}\")\n",
        "\n",
        "# Train final SVM with best params\n",
        "svm_best = ...\n",
        "...\n",
        "\n",
        "# Predictions\n",
        "y_val_svm = ...\n",
        "y_test_svm = ...\n",
        "\n",
        "# Metrics\n",
        "acc_val_svm = ...\n",
        "f1_val_svm = ...\n",
        "\n",
        "acc_test_svm = ...\n",
        "f1_test_svm = ...\n",
        "\n",
        "print(f\"\\nSVM Validation: acc={acc_val_svm:.4f}, f1(weighted)={f1_val_svm:.4f}\")\n",
        "print(f\"SVM Test: acc={acc_test_svm:.4f}, f1(weighted)={f1_test_svm:.4f}\")\n"
      ],
      "metadata": {
        "id": "QzCS5wZ_fNq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Multi-Layer Perceptron (MLP)\n",
        "\n",
        "Finally, letâ€™s explore how a **Multi-Layer Perceptron (MLP)** performs on our dataset.  \n",
        "Unlike simpler models, MLPs are powerful at handling **non-linear relationships** in complex feature spaces â€”  \n",
        "which is especially useful now that we have **150 PCA components** as input features.  \n",
        "\n",
        "By training an MLP, we can see how a neural network compares against **KNN** and **SVM** in terms of performance and training time.\n",
        "\n",
        "---\n",
        "\n",
        "# Exercise 5\n",
        "### Tasks\n",
        "\n",
        "1. Train an **MLP classifier** on the PCA-reduced dataset (150 features).  \n",
        "2. Tune key hyper-parameters (e.g., number of hidden units, layers, activation, learning rate) using the **Validation set**.  \n",
        "3. Evaluate the best model on the **Test set**, reporting **accuracy** and **weighted F1-score**.  \n",
        "4. Compare results to **KNN** and **SVM**:  \n",
        "   - Does MLP achieve better performance?  \n",
        "   - How does the training time differ?  \n",
        "5. Reflect on the strengths and weaknesses of using a neural network for this task.  \n"
      ],
      "metadata": {
        "id": "JhqJHdgYft4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- MLP hyperparameters to try ---\n",
        "hidden_layers = [(50,), (100,), (100,50)]\n",
        "alphas = [0.0001, 0.001]\n",
        "activations = ['relu', 'tanh', 'logistic']\n",
        "\n",
        "best_mlp_params = None\n",
        "best_f1_mlp = 0\n",
        "\n",
        "# Iterate over hyperparameters on validation set\n",
        "for hl in hidden_layers:\n",
        "    for alpha in alphas:\n",
        "        for activation in activations:\n",
        "            mlp_clf = MLPClassifier(\n",
        "                hidden_layer_sizes=hl,\n",
        "                activation=activation,\n",
        "                alpha=alpha,\n",
        "                max_iter=300,\n",
        "                early_stopping=True,\n",
        "                random_state=42\n",
        "            )\n",
        "            # TRAIN - SIMILAR TO PREVIOUS METHODS\n",
        "            ...\n",
        "\n",
        "            # VALIDATE\n",
        "            ...\n",
        "\n",
        "            # METRICS\n",
        "            f1_val = ...\n",
        "            acc_val = ...\n",
        "\n",
        "            print(f\"MLP hl={hl}, alpha={alpha}, act={activation}: Validation acc={acc_val:.4f}, f1(weighted)={f1_val:.4f}\")\n",
        "\n",
        "            if f1_val > best_f1_mlp:\n",
        "                best_f1_mlp = f1_val\n",
        "                best_mlp_params = {'hidden_layer_sizes': hl, 'alpha': alpha, 'activation': activation}\n",
        "\n",
        "print(f\"\\nBest MLP params based on validation F1: {best_mlp_params}\")\n",
        "\n",
        "# Train final MLP with best params\n",
        "mlp_best = ...\n",
        "...\n",
        "\n",
        "# Predictions\n",
        "y_val_mlp = ...\n",
        "y_test_mlp = ...\n",
        "\n",
        "# Metrics\n",
        "acc_val_mlp = ...\n",
        "f1_val_mlp = ...\n",
        "\n",
        "acc_test_mlp = ...\n",
        "f1_test_mlp = ...\n",
        "\n",
        "print(f\"\\nMLP Validation: acc={acc_val_mlp:.4f}, f1(weighted)={f1_val_mlp:.4f}\")\n",
        "print(f\"MLP Test: acc={acc_test_mlp:.4f}, f1(weighted)={f1_test_mlp:.4f}\")\n"
      ],
      "metadata": {
        "id": "HXBkdqKvgBs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Conclusions\n",
        "\n",
        "Our experiments showed that different models capture the datasetâ€™s structure in different ways:\n",
        "\n",
        "- **KNN**: Simple and intuitive, but struggles as feature spaces become complex and non-linear.  \n",
        "- **SVM**: More powerful than KNN, especially with the right kernel and tuned hyper-parameters, but still limited when data is noisy or classes overlap.  \n",
        "- **MLP**: Able to model non-linear relationships and generally achieves the best performance on this task.  \n",
        "\n",
        "That said, the MLP did not continue to improve indefinitely with increasing size or depth.  \n",
        "Performance plateaued around a relatively small architecture (â‰ˆ one hidden layer with ~100 neurons).  \n",
        "Adding more neurons or layers led to higher training cost without clear accuracy gains.  \n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "- **Complex models require sufficient data**: With too few samples and many imbalanced classes, additional model capacity does not translate into better generalization.  \n",
        "- **Trade-off between complexity and data availability**: More complex models can only shine when backed by enough representative training examples.  \n",
        "- **Practical implication**: For this dataset, simpler models (like SVM) already provide competitive performance, while MLP shows its strength but is held back by data limitations.  \n",
        "\n",
        "> In summary: **High model complexity should always be matched with larger, balanced datasets to unlock its full potential.**\n"
      ],
      "metadata": {
        "id": "YsYz8W7pgRSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6\n",
        "### Tasks\n",
        "1. Plot the best f1-score for each method side by side in a bar plot"
      ],
      "metadata": {
        "id": "ExxkhCs1U-3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CODE HERE\n",
        "# F1 scores from best-tuned models\n",
        "# FILL THE ... WITH THE NECESSARY VALUES\n",
        "results = {\n",
        "    'KNN': ...,\n",
        "    'SVM': ...,\n",
        "    'MLP': ...\n",
        "}\n",
        "\n",
        "# Plot F1 score as bars\n",
        "plt.figure(figsize=(8,6))\n",
        "bars = plt.bar(results.keys(), results.values(), color=['blue', 'green', 'red'])\n",
        "plt.ylabel(\"Weighted F1 Score\")\n",
        "plt.title(\"Classifier Comparison (with PCA)\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add F1 score value above each bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02, f\"{height:.2f}\",\n",
        "             ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VsncMQ_NgQq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play more (OPTIONAL)\n",
        "\n",
        "1. See how different image sizes affect the results.\n",
        "2. See how adding the other classes affects the results. Can increasing network size now help?\n",
        "3. Try mitigating the class imbalance issues with the dataset.\n",
        "4. Try data augmentation! Flip the images horizontaly, rotate them slightly, then flatten them and retrain."
      ],
      "metadata": {
        "id": "F4FGZ_M0YSZi"
      }
    }
  ]
}