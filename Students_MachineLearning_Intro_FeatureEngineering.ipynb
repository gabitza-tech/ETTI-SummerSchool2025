{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmw4yAWnpxBRKB9kufOCLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabitza-tech/ETTI-SummerSchool2025/blob/main/Students_MachineLearning_Intro_FeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 PART 1 - Introduction: Binary Classification with the Adult Income Dataset\n",
        "\n",
        "In this exercise, we will explore the **Adult Income** dataset, a widely-used dataset for **classification tasks**.  \n",
        "\n",
        "The main goal of this exercise is to:  \n",
        "- Gain a solid understanding of the dataset  \n",
        "- Preprocess and prepare it for a **binary classification** task  \n",
        "- Apply Machine Learning techniques to predict income levels  \n",
        "\n",
        "---\n",
        "\n",
        "### 🛠 Tools & Libraries\n",
        "We will primarily use **Scikit-Learn**, a powerful and versatile library for **Data Science** and **Machine Learning**. Some of its highlights:  \n",
        "\n",
        "- Ready-to-use datasets for **prototyping and experimentation**  \n",
        "- Built-in **data preprocessing tools**  \n",
        "- Wide selection of **Machine Learning algorithms**  \n",
        "- Easy evaluation with common metrics such as:  \n",
        "  - ✅ Accuracy  \n",
        "  - ✅ Precision  \n",
        "  - ✅ Recall  \n",
        "  - ✅ F1-score  \n",
        "\n",
        "Scikit-Learn makes it straightforward to experiment with different models, and while it works well out-of-the-box, understanding the **hyperparameters** is important for improving performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 💾 Saving Results\n",
        "All results, including **screenshots and brief explanations**, will be saved in Google Docs for documentation purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Step 1: Load the Dataset\n",
        "Let's start by loading the **Adult Income** dataset and exploring its structure.\n"
      ],
      "metadata": {
        "id": "sbX2b-CGfpJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "\n",
        "# Load Adult dataset\n",
        "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "x, y = adult.data, adult.target\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uldeYc37hCy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 1: Exploring the Dataset\n",
        "\n",
        "In this exercise, we will take a closer look at the dataset by examining the features (**X**) and the labels (**y**).  \n",
        "> **Hint:** The dataset is in **pandas DataFrame** format, so you can leverage all the familiar pandas methods.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Preview the data**: Display the first 5 samples in the dataset.\n",
        "2. **Dataset size**: How many samples are there in total?\n",
        "3. **Feature count**: How many features does each sample have?\n",
        "4. **Number of classes**: How many unique classes are present in the target variable?\n",
        "5. **Class distribution**: How many samples belong to each class?\n",
        "6. **Missing values**: Identify the total number of missing values.  \n",
        "   > **Hint:** Use the `isna()` method in pandas.\n",
        "7. **Missing value percentages**: Compute what percentage of each feature contains missing data.\n",
        "8. **Feature types**: Determine the type (categorical or numerical) of the features that contain missing values.\n",
        "\n",
        "---\n",
        "\n",
        "Take your time to explore the dataset thoroughly—this step is crucial for **data cleaning** and **preprocessing**, which can significantly impact model performance.\n"
      ],
      "metadata": {
        "id": "pQjjq2uthE_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values in the dataset are marked with '?' => Replace \"?\" with NaN\n",
        "print(type(x))\n",
        "print(type(y))\n",
        "x = x.replace(\"?\", pd.NA)\n",
        "\n",
        "# .... Code here\n",
        "# Print first 5 samples\n",
        "\n",
        "# Get the number of samples in the dataset (rows) and the number of features (columns)\n",
        "\n",
        "# How many classes?\n",
        "\n",
        "# Samples per calss\n",
        "\n",
        "# How many missing values do we have? => isna().sum()\n",
        "\n",
        "# Type of features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xhCpSp5FhKl_",
        "outputId": "e9d954f4-2e23-4b9a-83f1-2892455c030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n",
            "0   25    Private  226802          11th              7       Never-married   \n",
            "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
            "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
            "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
            "4   18        NaN  103497  Some-college             10       Never-married   \n",
            "\n",
            "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
            "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
            "1    Farming-fishing      Husband  White    Male             0             0   \n",
            "2    Protective-serv      Husband  White    Male             0             0   \n",
            "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
            "4                NaN    Own-child  White  Female             0             0   \n",
            "\n",
            "   hours-per-week native-country  \n",
            "0              40  United-States  \n",
            "1              50  United-States  \n",
            "2              40  United-States  \n",
            "3              40  United-States  \n",
            "4              30  United-States  \n",
            "(48842, 14)\n",
            "{'>50K', '<=50K'}\n",
            "['<=50K', '>50K']\n",
            "Categories (2, object): ['<=50K', '>50K']\n",
            "class\n",
            "<=50K    37155\n",
            ">50K     11687\n",
            "Name: count, dtype: int64\n",
            "age                  0\n",
            "workclass         2799\n",
            "fnlwgt               0\n",
            "education            0\n",
            "education-num        0\n",
            "marital-status       0\n",
            "occupation        2809\n",
            "relationship         0\n",
            "race                 0\n",
            "sex                  0\n",
            "capital-gain         0\n",
            "capital-loss         0\n",
            "hours-per-week       0\n",
            "native-country     857\n",
            "dtype: int64\n",
            "age               0.000000\n",
            "workclass         5.730724\n",
            "fnlwgt            0.000000\n",
            "education         0.000000\n",
            "education-num     0.000000\n",
            "marital-status    0.000000\n",
            "occupation        5.751198\n",
            "relationship      0.000000\n",
            "race              0.000000\n",
            "sex               0.000000\n",
            "capital-gain      0.000000\n",
            "capital-loss      0.000000\n",
            "hours-per-week    0.000000\n",
            "native-country    1.754637\n",
            "dtype: float64\n",
            "category\n",
            "category\n",
            "category\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 2: Handling Missing Data\n",
        "\n",
        "Now that we have identified the missing values in the dataset, and since they represent **≤5% of the samples**, we can remove them.  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Drop the samples with missing values from both **features (X)** and **labels (y)**.  \n",
        "   > **Hint:** Use `dropna()`.\n",
        "2. Check the **new size** of the dataset.\n",
        "3. Calculate **how much data was lost** after removing the missing entries.\n"
      ],
      "metadata": {
        "id": "Chaseekblewg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE HERE\n",
        "# Drop samples that have\n",
        "\n",
        "# Check new dataset size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6LQ2qKiFlcgg",
        "outputId": "f1fa958c-0e98-4783-c17c-ced65d8bcbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48842, 14)\n",
            "(45222, 14)\n",
            "7.411653904426519 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔄 Transforming Categorical Data into Numbers\n",
        "\n",
        "Many Machine Learning algorithms require **numerical inputs**, so we need to convert categorical features into numbers.  \n",
        "\n",
        "For this, we will use **`LabelEncoder`** from `scikit-learn`.  \n",
        "\n",
        "> This time, we will handle this step for you. Next time, you will be on your own! 🙂\n"
      ],
      "metadata": {
        "id": "ELyqM-avxJ_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Numerically encode features based on the number of unique values\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Separate categorical and numeric columns\n",
        "cat_cols = x_drop.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "num_cols = x_drop.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# 1. Label Encoder - Individual numerical values for each categorical value in a feature\n",
        "x_encoded = x_drop.copy()\n",
        "for col in cat_cols:\n",
        "  x_encoded[col] = le.fit_transform(x_drop[col])\n",
        "\n",
        "# x_encoded has the same structure as x, but with categorical columns having numerical values now\n",
        "x_encoded.head()\n",
        "\n",
        "# For labels, we can simply transform them in numerical values in the case of binary classification\n",
        "y_encoded = le.fit_transform(y_drop)\n",
        "print(set(y_encoded))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYEyKovzxKFR",
        "outputId": "b64e350a-ca1b-494a-fab9-bd701ca2546b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{np.int64(0), np.int64(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 3: Train-Test Split\n",
        "\n",
        "Next, we will split our dataset into a **training set** and a **test set**.  \n",
        "\n",
        "> In real-world applications, we would usually also create a **validation set**, but for this introductory exercise, we will keep it simple with just two splits.  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Split the data into **train (70%)** and **test (30%)** sets using `train_test_split()`.  \n",
        "   > **Note:** Use the `stratify` parameter to ensure class proportions are preserved.\n",
        "2. Check the **number of samples** in the train and test sets.\n",
        "3. Verify that the **class distribution** is balanced in both sets.  \n",
        "   - Example: If the training set has 65% `'>=50K'` and 35% `'<=50K'`, the test set should have a **similar distribution**.\n"
      ],
      "metadata": {
        "id": "DlRwQ7nDocOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Complete the function - search it\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_encoded, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42)\n",
        "\n",
        "# CODE HERE - STUDENTS\n",
        "# No samples\n",
        "...\n",
        "\n",
        "# See if train-test balanced\n",
        "..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hNuhgP55ocBA",
        "outputId": "fab6eb00-2106-4fd7-b8ef-4bdd424256bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31655\n",
            "13567\n",
            "y_train counts:\n",
            "0: 23809\n",
            "1: 7846\n",
            "\n",
            "y_test counts:\n",
            "0: 10205\n",
            "1: 3362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Training & Inference with a Simple Logistic Regression\n",
        "\n",
        "Now that our data is **cleaned**, **encoded**, and **split** into training and test sets, we can move on to **making predictions**.  \n",
        "\n",
        "In this section, we will train a **K Nearest Neighbour** classifier and a **Logistic Regression** model, two of the simplest and most widely used algorithms for **binary classification**, and then evaluate its performance on the test set.\n",
        "\n",
        "# 📝 Exercise 4: Comparing 2 Machine Learning Classification Methods\n",
        "### Tasks\n",
        "\n",
        "1. Which algorithm has the better performance?\n",
        "2. Which algorithm is faster to train? Do some profiling on training time taken for each.\n",
        "3. How does changing the `n_neighbors` parameter for KNN affect results and how does `max_interations` affect the results?\n",
        "4. Why isn't it ok to simply change the training hyper-parameters of the methods and then evaluate them on the test set?"
      ],
      "metadata": {
        "id": "IP-CsSNws2Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "# --- Make predictions ---\n",
        "y_pred = knn.predict(x_test)\n",
        "\n",
        "print(\"\\nKNN Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(x_test)\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Alternatively, just for checking accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYhfH3Fis4kW",
        "outputId": "e5d776ad-1665-4ef4-fc32-345456450ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "KNN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85     10205\n",
            "           1       0.54      0.32      0.40      3362\n",
            "\n",
            "    accuracy                           0.76     13567\n",
            "   macro avg       0.67      0.62      0.63     13567\n",
            "weighted avg       0.74      0.76      0.74     13567\n",
            "\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89     10205\n",
            "           1       0.72      0.55      0.62      3362\n",
            "\n",
            "    accuracy                           0.83     13567\n",
            "   macro avg       0.79      0.74      0.76     13567\n",
            "weighted avg       0.83      0.83      0.83     13567\n",
            "\n",
            "Accuracy: 0.8343775337215302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📏 Scaling Numerical Features\n",
        "\n",
        "When working with numerical data, it’s important to **scale features** so that they have a similar distribution.  \n",
        "For example, scaling values to a **range between 0 and 1** can improve the performance of many Machine Learning algorithms.\n",
        "\n",
        "Scikit-Learn provides built-in scalers, so we don’t have to implement them manually.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Scaling Techniques\n",
        "\n",
        "**1. StandardScaler**  \n",
        "Scales each feature to have **mean** $\\mu = 0$ and **standard deviation** $\\sigma = 1$:  \n",
        "\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "**2. MinMaxScaler**  \n",
        "Scales each feature to a specified range, by default \\([0, 1]\\):  \n",
        "\n",
        "$$\n",
        "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "\n",
        "> You can also choose other ranges, e.g., \\([-1, 1]\\).\n",
        "\n",
        "---\n",
        "\n",
        "**⚠️ Important:** Always **FIT** your scaler on the **TRAINING set** only.  \n",
        "Scaling the train and test set together can lead to **data leakage**, which will make your results **skewed, biased, and unfair**.\n",
        "\n",
        "E.g.: 'centering the values of the dataset with the mean of the entire dataset' -> can lead to influencing training with the distribution of the test set, which might be totally different.\n"
      ],
      "metadata": {
        "id": "ntLCkMWG0C9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 5: Scaling and Its Impact\n",
        "\n",
        "In this exercise, we will explore how **different scaling methods** affect model performance.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Scale your data using:  \n",
        "   - `StandardScaler()`  \n",
        "   - `MinMaxScaler()`  \n",
        "   > **Hint:** Use `fit_transform()` on the training data and `transform()` on the test data.\n",
        "2. Verify that your scaled data **looks different** from the original data.  \n",
        "   > It’s always good to double-check that scaling was applied correctly.\n",
        "3. Train **two separate Logistic Regression models**:  \n",
        "   - One on the StandardScaler data  \n",
        "   - One on the MinMaxScaler data  \n",
        "4. Compare the results. Which scaling method gives **better performance** on the test set?\n"
      ],
      "metadata": {
        "id": "NsbIAxdB_bFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# CODE HERE\n",
        "scaler_std = StandardScaler()\n",
        "X_train_std = scaler_std.fit_transform(x_train)\n",
        "X_test_std = scaler_std.transform(x_test)\n",
        "\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(x_train)\n",
        "X_test_minmax = scaler_minmax.transform(x_test)\n",
        "\n",
        "print(X_train_std[:2])\n",
        "print('---')\n",
        "print(X_train_minmax[:2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yo2nr8XN0Byb",
        "outputId": "73f3bd94-cac2-416a-ba0d-e281446eb1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.36395269e-01 -5.16254212e-01  1.51020289e+00 -1.48057352e-01\n",
            "  -2.18400256e-01  7.47278750e-01 -1.77512502e-01 -2.70817140e-01\n",
            "   5.99256763e-01 -1.94161628e-01 -3.04870748e-01 -2.13176071e-01\n",
            "  -2.24878853e-02 -1.65709108e-01 -1.96080747e-01 -1.13128035e-01\n",
            "  -7.17216861e-02 -9.89610441e-02 -1.38288322e-01 -1.22234059e-01\n",
            "  -1.85862157e-01 -2.15433723e-01 -4.49508816e-01 -1.10814024e-01\n",
            "  -6.90250603e-01  4.08949437e+00 -4.13377150e-02 -1.34685923e-01\n",
            "  -5.29075373e-01  2.46931846e+00 -2.63718836e-02 -9.31601965e-01\n",
            "  -1.10375014e-01 -6.88256102e-01 -1.81543571e-01 -1.70968035e-01\n",
            "  -3.71997879e-01 -1.48722229e-02 -3.91177193e-01  2.55463366e+00\n",
            "  -1.84493434e-01 -2.16235319e-01 -2.66763371e-01 -3.45058394e-01\n",
            "  -6.87696003e-02 -3.92946824e-01 -1.47384838e-01 -3.71392345e-01\n",
            "  -1.77133437e-01 -2.31487121e-01 -8.38493911e-01  1.68681937e+00\n",
            "  -1.74171120e-01 -4.13964000e-01 -3.44542244e-01 -2.17751670e-01\n",
            "  -9.91236684e-02 -1.72138892e-01 -3.21355261e-01 -8.72214910e-02\n",
            "   4.02906748e-01  1.44321823e+00 -1.44321823e+00 -2.63718836e-02\n",
            "  -6.19445573e-02 -5.12729138e-02 -4.32125558e-02 -5.48647375e-02\n",
            "  -4.70769783e-02 -3.13092343e-02 -5.82379357e-02 -5.45743472e-02\n",
            "  -2.81138555e-02 -6.22009861e-02 -3.64495180e-02 -4.01711312e-02\n",
            "  -3.64495180e-02 -5.62063911e-03 -2.17734572e-02 -2.45067667e-02\n",
            "  -1.94738490e-02 -5.93202684e-02 -3.32700221e-02 -2.97543191e-02\n",
            "  -4.90577482e-02 -4.93802236e-02 -4.67387647e-02 -2.10348257e-02\n",
            "  -1.44563158e-01 -3.37424944e-02 -2.10348257e-02 -2.97543191e-02\n",
            "  -7.70879031e-02 -4.50099261e-02 -3.73084512e-02 -6.22009861e-02\n",
            "  -2.31803479e-02 -4.63981074e-02 -3.81481549e-02 -2.81138555e-02\n",
            "  -2.17734572e-02  3.12300141e-01 -4.39402588e-02 -2.31803479e-02]\n",
            " [ 1.77064060e+00  5.24994578e-01  1.12030743e+00  1.30818124e+01\n",
            "  -2.18400256e-01 -8.44000990e-02 -1.77512502e-01 -2.70817140e-01\n",
            "  -1.66873377e+00  5.15034824e+00 -3.04870748e-01 -2.13176071e-01\n",
            "  -2.24878853e-02 -1.65709108e-01 -1.96080747e-01 -1.13128035e-01\n",
            "  -7.17216861e-02 -9.89610441e-02 -1.38288322e-01 -1.22234059e-01\n",
            "  -1.85862157e-01 -2.15433723e-01  2.22465047e+00 -1.10814024e-01\n",
            "  -6.90250603e-01 -2.44529008e-01 -4.13377150e-02 -1.34685923e-01\n",
            "  -5.29075373e-01  2.46931846e+00 -2.63718836e-02 -9.31601965e-01\n",
            "  -1.10375014e-01 -6.88256102e-01 -1.81543571e-01 -1.70968035e-01\n",
            "  -3.71997879e-01 -1.48722229e-02 -3.91177193e-01 -3.91445559e-01\n",
            "  -1.84493434e-01 -2.16235319e-01 -2.66763371e-01 -3.45058394e-01\n",
            "  -6.87696003e-02 -3.92946824e-01 -1.47384838e-01  2.69257030e+00\n",
            "  -1.77133437e-01 -2.31487121e-01 -8.38493911e-01  1.68681937e+00\n",
            "  -1.74171120e-01 -4.13964000e-01 -3.44542244e-01 -2.17751670e-01\n",
            "  -9.91236684e-02 -1.72138892e-01 -3.21355261e-01 -8.72214910e-02\n",
            "   4.02906748e-01 -6.92895906e-01  6.92895906e-01 -2.63718836e-02\n",
            "  -6.19445573e-02 -5.12729138e-02 -4.32125558e-02 -5.48647375e-02\n",
            "  -4.70769783e-02 -3.13092343e-02 -5.82379357e-02 -5.45743472e-02\n",
            "  -2.81138555e-02 -6.22009861e-02 -3.64495180e-02 -4.01711312e-02\n",
            "  -3.64495180e-02 -5.62063911e-03 -2.17734572e-02 -2.45067667e-02\n",
            "  -1.94738490e-02 -5.93202684e-02 -3.32700221e-02 -2.97543191e-02\n",
            "  -4.90577482e-02 -4.93802236e-02 -4.67387647e-02 -2.10348257e-02\n",
            "  -1.44563158e-01 -3.37424944e-02 -2.10348257e-02 -2.97543191e-02\n",
            "  -7.70879031e-02 -4.50099261e-02 -3.73084512e-02 -6.22009861e-02\n",
            "  -2.31803479e-02 -4.63981074e-02 -3.81481549e-02 -2.81138555e-02\n",
            "  -2.17734572e-02  3.12300141e-01 -4.39402588e-02 -2.31803479e-02]]\n",
            "---\n",
            "[[0.35616438 0.08282957 0.86666667 0.         0.         0.5\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.        ]\n",
            " [0.61643836 0.15753371 0.8        1.         0.         0.39795918\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Train Logistic Regression StandardScaler---\n",
        "\n",
        "\n",
        "# --- Make predictions ---\n",
        "y_pred_std = ...\n",
        "print(\"\\nStandard Scaling Classification Report:\\n\", classification_report(...))\n",
        "\n",
        "# --- Train Logistic Regression MinMaxScaler---\n",
        "\n",
        "\n",
        "# --- Make predictions ---\n",
        "y_pred_minmax = ...\n",
        "print(\"\\nMin-Max Scaling Classification Report:\\n\", classification_report(..))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egwfQbql9GKG",
        "outputId": "a526d263-3f25-4cba-cb30-81855d899e99",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Standard Scaling Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90     10205\n",
            "           1       0.73      0.59      0.65      3362\n",
            "\n",
            "    accuracy                           0.84     13567\n",
            "   macro avg       0.80      0.76      0.78     13567\n",
            "weighted avg       0.84      0.84      0.84     13567\n",
            "\n",
            "\n",
            "Min-Max Scaling Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90     10205\n",
            "           1       0.73      0.58      0.65      3362\n",
            "\n",
            "    accuracy                           0.84     13567\n",
            "   macro avg       0.80      0.76      0.77     13567\n",
            "weighted avg       0.84      0.84      0.84     13567\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 PART 2 - Transforming Categorical Data with One-Hot Encoding\n",
        "\n",
        "Many Machine Learning algorithms require **numerical input**, so categorical features must be converted into numbers.  \n",
        "\n",
        "This time, we will use **`OneHotEncoder`** from `scikit-learn`, which creates a **binary column for each category** in a feature.  \n",
        "\n",
        "> Good news: You already have the One-Hot Encoded data ready as `X_encoded` and `y_encoded`. (lucky you!)  \n",
        "\n",
        "However, you will need to **repeat the previous steps** with this new encoding:  \n",
        "- Split the data into train and test sets  \n",
        "- Scale the numerical features if necessary  \n",
        "- Train the model  \n",
        "- Compare the results with the previous **LabelEncoder** approach  \n",
        "\n",
        "This will help you understand **how encoding choices impact model performance**.\n"
      ],
      "metadata": {
        "id": "Knx79Ufa8B3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# One-Hot Encode Features\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "\n",
        "# Separate categorical and numeric columns\n",
        "cat_cols = x_drop.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "num_cols = x_drop.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# One-Hot Encode categorical features - Each feature, will be One-Hot Encoded\n",
        "x_encoded_array = ohe.fit_transform(x_drop[cat_cols])\n",
        "\n",
        "# Convert encoded array back to DataFrame\n",
        "x_encoded_df = pd.DataFrame(\n",
        "    x_encoded_array,\n",
        "    columns=ohe.get_feature_names_out(cat_cols),\n",
        "    index=x_drop.index\n",
        ")\n",
        "\n",
        "# Combine numeric columns and encoded categorical columns\n",
        "x_encoded = pd.concat([x_drop[num_cols], x_encoded_df], axis=1)\n",
        "\n",
        "# x_encoded now has more columns than the original x, but with categorical columns one-hot encoded\n",
        "x_encoded.head()\n",
        "\n",
        "# For labels, we can simply transform them in numerical values in the case of binary classification\n",
        "y_encoded = le.fit_transform(y_drop)\n",
        "print(set(y_encoded))\n"
      ],
      "metadata": {
        "id": "i80sQXHw8CBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 6: One-Hot Encoding and Its Impact\n",
        "\n",
        "In this exercise, we will evaluate how **One-Hot Encoding** affects model performance compared to Label Encoding.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Determine **how many features** the One-Hot Encoded dataset now contains.\n",
        "2. Split the data into **train and test sets**, keeping the **same proportions** as in the first case for a fair comparison.\n",
        "3. Train **two Logistic Regression classifiers**:  \n",
        "   - One with scaled features  \n",
        "   - One without scaling\n",
        "4. Compare the results and analyze:  \n",
        "   - What are the **performance gains or losses** with One-Hot Encoding compared to Label Encoding?\n",
        "5. Is training faster or slower than previously? Do some profiling of time taken for training."
      ],
      "metadata": {
        "id": "kfc6SmPs872S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "0fOKT9JL9YmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 7 (Optional - Homework): Handling Missing Values\n",
        "\n",
        "For the curious minds: so far, we simply **dropped samples** with missing values.  \n",
        "But what if we **keep them** and try to fill in the missing information?  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Treat the missing values (`'n/a'`) as a **separate category** for categorical features.  \n",
        "   > Hint: This is very easy to do in pandas.\n",
        "2. Fill the missing values:  \n",
        "   - **Categorical features:** use the **most frequent value** in the column  \n",
        "   - **Numerical features:** use the **mean or median** of the column\n",
        "3. Train your model again and **compare the results**.  \n",
        "   - Does filling missing values improve performance, or not?\n"
      ],
      "metadata": {
        "id": "5oa0I32GBPCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "_JIWUojtB8xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Exercise 8 (Optional - Homework): Exploring Interesting Relationships\n",
        "\n",
        "At the end of the day, the goal of data analysis and predictions is to **generate insights** that can have a real-world impact.  \n",
        "This exercise encourages you to explore **interesting patterns and relationships** in the Adult Income dataset.\n",
        "\n",
        "### Suggested Questions to Explore\n",
        "\n",
        "1. Where do people earning **>=50K/year** come from?  \n",
        "2. How many hours do they work in each occupation or category?  \n",
        "3. What is the **average age** in each category?  \n",
        "4. How is the **gender balance** for each income category?  \n",
        "5. How many hours do people in each category work?  \n",
        "6. Any **other observations** that you find interesting or surprising.  \n",
        "\n",
        "> Feel free to use **groupby**, **pivot tables**, or **visualizations** to uncover meaningful patterns.\n"
      ],
      "metadata": {
        "id": "_1miRc-LXk3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here"
      ],
      "metadata": {
        "id": "175_18WaX297"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}